{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "testPytorch.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMNMJeP6UaN1fwj6/aeo0LW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/titiac/testPytorch/blob/main/testPytorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8VlRWVDqsDqz"
      },
      "outputs": [],
      "source": [
        "# 一个使用relu激活函数的样例\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "# 两层神经网络， 输入64个个体， 10 应该十个分类\n",
        "N, D_in, H, D_out = 64, 1000, 100, 10\n",
        "\n",
        "# x 输入层  y 输出层\n",
        "x = np.random.randn(N, D_in)\n",
        "y = np.random.randn(N, D_out)\n",
        "\n",
        "# w1 隐含层权重, w2输出层权重 \n",
        "w1 = np.random.randn(D_in, H)\n",
        "w2 = np.random.randn(H, D_out)\n",
        "\n",
        "learning_rate = 1e-6  # 学习率\n",
        "for t in range(500):   \n",
        "  h = x.dot(w1)\n",
        "  h_relu = np.maximum(h, 0) # relu 激活\n",
        "  y_pred = h_relu.dot(w2) # 输出\n",
        "\n",
        "  loss = np.square(y_pred - y).sum()  # 对于正确标签的损失的损失\n",
        "  print(t, loss)\n",
        "\n",
        "  grad_y_pred = 2.0 * (y_pred - y)\n",
        "  grad_w2 = h_relu.T.dot(grad_y_pred)  # 应该都是loss对grad进行求导，那么为啥是这样进行反向传播的，需要手动推？\n",
        "  grad_h_relu = grad_y_pred.dot(w2.T)\n",
        "  grad_h = grad_h_relu.copy()\n",
        "  grad_h[h < 0] = 0\n",
        "  grad_w1 = x.T.dot(grad_h)\n",
        " \n",
        "  # Update weights\n",
        "  w1 -= learning_rate * grad_w1\n",
        "  w2 -= learning_rate * grad_w2\n",
        "\n",
        "  # if(t == 499):\n",
        "  #   print(f\"x: \\n{x}\\n\")\n",
        "  #   print(f\"h: \\n{h}\\n\")\n",
        "  #   print(f\"h_relu: \\n{h_relu}\\n\")\n",
        "  #   print(f\"y_pred: \\n{y_pred}\\n\")\n",
        "  #   print(f\"loss: \\n{loss}\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Pytorch 导引\n",
        "**Numpy** 是一个很好的框架，但它不能利用GPU进行加速，对于现代神经网络来说，GPU通常提供50倍或更高的加速，因此**Numpy**不适用于现代深度学习。\n",
        "\n",
        "**Pytorch**的张量在概念上与$numpy$数组相同：张量是一个n维数组，$pytorch$也提供了很多对这些张量进行操作的函数。常见的$numpy$计算都可以用$pytorch$张量来完成。\n",
        "\n",
        "与$numpy$不同的是，$pytorch$可以利用GPU进行加速其计算。\n",
        "\n",
        "下面是手动模拟两层神经网络使用$pytorch$张量计算,\n",
        "下面可能使用CPU还会比GPU快\n"
      ],
      "metadata": {
        "id": "w6cGq7b3W_jN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Code in file autograd/two_layer_net_autograd.py\n",
        "import torch\n",
        "\n",
        "device = torch.device('cpu')\n",
        "# device = torch.device('cuda') # Uncomment this to run on GPU  如果将该注释取消掉的话， 设备将会变为GPU\n",
        "\n",
        "# N is batch size; D_in is input dimension;  \n",
        "# H is hidden dimension; D_out is output dimension.\n",
        "N, D_in, H, D_out = 64, 1000, 100, 10\n",
        "\n",
        "# Create random Tensors to hold input and outputs\n",
        "x = torch.randn(N, D_in, device=device)\n",
        "y = torch.randn(N, D_out, device=device)\n",
        "\n",
        "# Create random Tensors for weights; setting requires_grad=True means that we\n",
        "# want to compute gradients for these Tensors during the backward pass.\n",
        "w1 = torch.randn(D_in, H, device=device, requires_grad=True)\n",
        "w2 = torch.randn(H, D_out, device=device, requires_grad=True)\n",
        "\n",
        "learning_rate = 1e-6\n",
        "for t in range(500):\n",
        "  # Forward pass: compute predicted y using operations on Tensors. Since w1 and           # 前向传播：使用张量运算计算y的预测值，当权重w1和w2的参数requires_grad=True，\n",
        "  # w2 have requires_grad=True, operations involving these Tensors will cause             ## 涉及这些张量的计算将会促使Pytorch生成一个计算图， 这个计算图可以让 \n",
        "  # PyTorch to build a computational graph, allowing automatic computation of             # 计算反向传播变得自动化，也就是脱离了手动计算反向传播\n",
        "  # gradients. Since we are no longer implementing the backward pass by hand we\n",
        "  # don't need to keep references to intermediate values.\n",
        "  y_pred = x.mm(w1).clamp(min=0).mm(w2)     # mm就类似与np.dot，clamp(min=0) relu激活函数的实现方式，\n",
        "  \n",
        "  # Compute and print loss. Loss is a Tensor of shape (), and loss.item()\n",
        "  # is a Python number giving its value.\n",
        "  loss = (y_pred - y).pow(2).sum()\n",
        "  print(t, loss.item())      # item()的作用是取出单元素张量的元素值并返回该值，保持该元素类型不变。 直接取也可以，但是item取出来的精度更高\n",
        "\n",
        "  # Use autograd to compute the backward pass. This call will compute the       # 使用自动微分去计算反向传播， 这个调用将计算\n",
        "  # gradient of loss with respect to all Tensors with requires_grad=True.        # 所有带有 requires_grad=True. 参数的张量的损失梯度   \n",
        "  # After this call w1.grad and w2.grad will be Tensors holding the gradient     ## 在调用函数之后， w1.grad 和 w2.grad 将分别保存 \n",
        "  # of the loss with respect to w1 and w2 respectively.                 # 损失值对于w1 和 w2的梯度\n",
        "  loss.backward()\n",
        "\n",
        "  # Update weights using gradient descent. For this step we just want to mutate    # 使用梯度下降更新梯度, 对于这个步骤我们只想就地转变w1和w2的值\n",
        "  # the values of w1 and w2 in-place; we don't want to build up a computational   # 我们不想为更新步骤构建计算图, 所以我们使用上下文管理器，\n",
        "  # graph for the update steps, so we use the torch.no_grad() context manager     ## 来阻止Pytorch 为了更新计算图而构建计算图\n",
        "  # to prevent PyTorch from building a computational graph for the updates       ##\n",
        "  with torch.no_grad():\n",
        "    w1 -= learning_rate * w1.grad\n",
        "    w2 -= learning_rate * w2.grad\n",
        "\n",
        "    # Manually zero the gradients after running the backward pass    # 运行后就要将梯度置为0\n",
        "    w1.grad.zero_()\n",
        "    w2.grad.zero_()"
      ],
      "metadata": {
        "id": "sccqmVyht4TJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Code in file autograd/two_layer_net_custom_function.py\n",
        "import torch\n",
        "\n",
        "class MyReLU(torch.autograd.Function):\n",
        "  \"\"\"\n",
        "  We can implement our own custom autograd Functions by subclassing            ## 我们可以通过子类化 torch.autograd函数 来实现我们自己的自定义\n",
        "  torch.autograd.Function and implementing the forward and backward passes          # autograd函数 和实现前向传播和反向传播操作张量。 \n",
        "  which operate on Tensors.\n",
        "  \"\"\"\n",
        "  @staticmethod\n",
        "  def forward(ctx, x):  # ctx 就是context 翻译成 \"上下文或者环境\" 常用于静态环境\n",
        "    \"\"\"\n",
        "    In the forward pass we receive a context object and a Tensor containing the      #  在前向传播中我们得到了一个上下文对象和一个包含输入的张量\n",
        "    input; we must return a Tensor containing the output, and we can use the       ##  我们必须返回一个包含输出的张量的输出， 并且我们能够使用这个\n",
        "    context object to cache objects for use in the backward pass.              ##  上下文对象去缓存用于反向传播的对象\n",
        "    \"\"\"\n",
        "    ctx.save_for_backward(x)\n",
        "    return x.clamp(min=0)\n",
        "\n",
        "  @staticmethod\n",
        "  def backward(ctx, grad_output):\n",
        "    \"\"\"\n",
        "    In the backward pass we receive the context object and a Tensor containing\n",
        "    the gradient of the loss with respect to the output produced during the\n",
        "    forward pass. We can retrieve cached data from the context object, and must\n",
        "    compute and return the gradient of the loss with respect to the input to the\n",
        "    forward function.\n",
        "    \"\"\"\n",
        "    x, = ctx.saved_tensors\n",
        "    grad_x = grad_output.clone()\n",
        "    grad_x[x < 0] = 0\n",
        "    return grad_x\n",
        "\n",
        "\n",
        "device = torch.device('cpu')\n",
        "# device = torch.device('cuda') # Uncomment this to run on GPU\n",
        "\n",
        "# N is batch size; D_in is input dimension;\n",
        "# H is hidden dimension; D_out is output dimension.\n",
        "N, D_in, H, D_out = 64, 1000, 100, 10\n",
        "\n",
        "# Create random Tensors to hold input and output\n",
        "x = torch.randn(N, D_in, device=device)\n",
        "y = torch.randn(N, D_out, device=device)\n",
        "\n",
        "# Create random Tensors for weights.\n",
        "w1 = torch.randn(D_in, H, device=device, requires_grad=True)\n",
        "w2 = torch.randn(H, D_out, device=device, requires_grad=True)\n",
        "\n",
        "learning_rate = 1e-6\n",
        "for t in range(500):\n",
        "  # Forward pass: compute predicted y using operations on Tensors; we call our\n",
        "  # custom ReLU implementation using the MyReLU.apply function\n",
        "  y_pred = MyReLU.apply(x.mm(w1)).mm(w2)\n",
        " \n",
        "  # Compute and print loss\n",
        "  loss = (y_pred - y).pow(2).sum()\n",
        "  print(t, loss.item())\n",
        "\n",
        "  # Use autograd to compute the backward pass.\n",
        "  loss.backward()\n",
        "\n",
        "  with torch.no_grad():\n",
        "    # Update weights using gradient descent\n",
        "    w1 -= learning_rate * w1.grad\n",
        "    w2 -= learning_rate * w2.grad\n",
        "\n",
        "    # Manually zero the gradients after running the backward pass\n",
        "    w1.grad.zero_()\n",
        "    w2.grad.zero_()\n"
      ],
      "metadata": {
        "id": "nF376niC9bh5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}