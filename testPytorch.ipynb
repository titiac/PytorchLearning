{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "testPytorch.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOlMXfNNsr2WLxgsFGxpsU6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/titiac/testPytorch/blob/main/testPytorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Warm-up:numpy"
      ],
      "metadata": {
        "id": "C-Zt6EAB-NR6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8VlRWVDqsDqz"
      },
      "outputs": [],
      "source": [
        "# 一个使用relu激活函数的样例\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "# 两层神经网络， 输入64个个体， 10 应该十个分类\n",
        "N, D_in, H, D_out = 64, 1000, 100, 10\n",
        "\n",
        "# x 输入层  y 输出层\n",
        "x = np.random.randn(N, D_in)\n",
        "y = np.random.randn(N, D_out)\n",
        "\n",
        "# w1 隐含层权重, w2输出层权重 \n",
        "w1 = np.random.randn(D_in, H)\n",
        "w2 = np.random.randn(H, D_out)\n",
        "\n",
        "learning_rate = 1e-6  # 学习率\n",
        "for t in range(500):   \n",
        "  h = x.dot(w1)\n",
        "  h_relu = np.maximum(h, 0) # relu 激活\n",
        "  y_pred = h_relu.dot(w2) # 输出\n",
        "\n",
        "  loss = np.square(y_pred - y).sum()  # 对于正确标签的损失的损失\n",
        "  print(t, loss)\n",
        "\n",
        "  grad_y_pred = 2.0 * (y_pred - y)\n",
        "  grad_w2 = h_relu.T.dot(grad_y_pred)  # 应该都是loss对grad进行求导，那么为啥是这样进行反向传播的，需要手动推？\n",
        "  grad_h_relu = grad_y_pred.dot(w2.T)\n",
        "  grad_h = grad_h_relu.copy()\n",
        "  grad_h[h < 0] = 0\n",
        "  grad_w1 = x.T.dot(grad_h)\n",
        " \n",
        "  # Update weights\n",
        "  w1 -= learning_rate * grad_w1\n",
        "  w2 -= learning_rate * grad_w2\n",
        "\n",
        "  # if(t == 499):\n",
        "  #   print(f\"x: \\n{x}\\n\")\n",
        "  #   print(f\"h: \\n{h}\\n\")\n",
        "  #   print(f\"h_relu: \\n{h_relu}\\n\")\n",
        "  #   print(f\"y_pred: \\n{y_pred}\\n\")\n",
        "  #   print(f\"loss: \\n{loss}\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Pytorch 导引\n",
        "**Numpy** 是一个很好的框架，但它不能利用GPU进行加速，对于现代神经网络来说，GPU通常提供50倍或更高的加速，因此**Numpy**不适用于现代深度学习。\n",
        "\n",
        "**Pytorch**的张量在概念上与$numpy$数组相同：张量是一个n维数组，$pytorch$也提供了很多对这些张量进行操作的函数。常见的$numpy$计算都可以用$pytorch$张量来完成。\n",
        "\n",
        "与$numpy$不同的是，$pytorch$可以利用GPU进行加速其计算。\n",
        "\n",
        "下面是手动模拟两层神经网络使用$pytorch$张量计算,\n",
        "下面可能使用CPU还会比GPU快\n"
      ],
      "metadata": {
        "id": "w6cGq7b3W_jN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PyTorch: Tensors"
      ],
      "metadata": {
        "id": "x7fcWldy-Vza"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Code in file tensor/two_layer_net_tensor.py\n",
        "import torch\n",
        "\n",
        "device = torch.device('cpu')\n",
        "# device = torch.device('cuda') # Uncomment this to run on GPU\n",
        "\n",
        "# N is batch size; D_in is input dimension;\n",
        "# H is hidden dimension; D_out is output dimension.\n",
        "N, D_in, H, D_out = 64, 1000, 100, 10\n",
        "\n",
        "# Create random input and output data\n",
        "x = torch.randn(N, D_in, device=device)\n",
        "y = torch.randn(N, D_out, device=device)\n",
        "\n",
        "# Randomly initialize weights\n",
        "w1 = torch.randn(D_in, H, device=device)\n",
        "w2 = torch.randn(H, D_out, device=device)\n",
        "\n",
        "learning_rate = 1e-6\n",
        "for t in range(500):\n",
        "  # Forward pass: compute predicted y\n",
        "  h = x.mm(w1)\n",
        "  h_relu = h.clamp(min=0)\n",
        "  y_pred = h_relu.mm(w2)\n",
        "\n",
        "  # Compute and print loss; loss is a scalar, and is stored in a PyTorch Tensor\n",
        "  # of shape (); we can get its value as a Python number with loss.item().\n",
        "  loss = (y_pred - y).pow(2).sum()\n",
        "  print(t, loss.item())\n",
        "\n",
        "  # Backprop to compute gradients of w1 and w2 with respect to loss\n",
        "  grad_y_pred = 2.0 * (y_pred - y)\n",
        "  grad_w2 = h_relu.t().mm(grad_y_pred)\n",
        "  grad_h_relu = grad_y_pred.mm(w2.t())\n",
        "  grad_h = grad_h_relu.clone()\n",
        "  grad_h[h < 0] = 0\n",
        "  grad_w1 = x.t().mm(grad_h)\n",
        "\n",
        "  # Update weights using gradient descent\n",
        "  w1 -= learning_rate * grad_w1\n",
        "  w2 -= learning_rate * grad_w2"
      ],
      "metadata": {
        "id": "wC_oxWPV8r1T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PyTorch: Autograd"
      ],
      "metadata": {
        "id": "gvVu4VpF-Z6b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Code in file autograd/two_layer_net_autograd.py\n",
        "import torch\n",
        "\n",
        "device = torch.device('cpu')\n",
        "# device = torch.device('cuda') # Uncomment this to run on GPU  如果将该注释取消掉的话， 设备将会变为GPU\n",
        "\n",
        "# N is batch size; D_in is input dimension;  \n",
        "# H is hidden dimension; D_out is output dimension.\n",
        "N, D_in, H, D_out = 64, 1000, 100, 10\n",
        "\n",
        "# Create random Tensors to hold input and outputs\n",
        "x = torch.randn(N, D_in, device=device)\n",
        "y = torch.randn(N, D_out, device=device)\n",
        "\n",
        "# Create random Tensors for weights; setting requires_grad=True means that we\n",
        "# want to compute gradients for these Tensors during the backward pass.\n",
        "w1 = torch.randn(D_in, H, device=device, requires_grad=True)\n",
        "w2 = torch.randn(H, D_out, device=device, requires_grad=True)\n",
        "\n",
        "learning_rate = 1e-6\n",
        "for t in range(500):\n",
        "  # Forward pass: compute predicted y using operations on Tensors. Since w1 and           # 前向传播：使用张量运算计算y的预测值，当权重w1和w2的参数requires_grad=True，\n",
        "  # w2 have requires_grad=True, operations involving these Tensors will cause             ## 涉及这些张量的计算将会促使Pytorch生成一个计算图， 这个计算图可以让 \n",
        "  # PyTorch to build a computational graph, allowing automatic computation of             # 计算反向传播变得自动化，也就是脱离了手动计算反向传播\n",
        "  # gradients. Since we are no longer implementing the backward pass by hand we\n",
        "  # don't need to keep references to intermediate values.\n",
        "  y_pred = x.mm(w1).clamp(min=0).mm(w2)     # mm就类似与np.dot，clamp(min=0) relu激活函数的实现方式，\n",
        "  \n",
        "  # Compute and print loss. Loss is a Tensor of shape (), and loss.item()\n",
        "  # is a Python number giving its value.\n",
        "  loss = (y_pred - y).pow(2).sum()\n",
        "  print(t, loss.item())      # item()的作用是取出单元素张量的元素值并返回该值，保持该元素类型不变。 直接取也可以，但是item取出来的精度更高\n",
        "\n",
        "  # Use autograd to compute the backward pass. This call will compute the       # 使用自动微分去计算反向传播， 这个调用将计算\n",
        "  # gradient of loss with respect to all Tensors with requires_grad=True.        # 所有带有 requires_grad=True. 参数的张量的损失梯度   \n",
        "  # After this call w1.grad and w2.grad will be Tensors holding the gradient     ## 在调用函数之后， w1.grad 和 w2.grad 将分别保存 \n",
        "  # of the loss with respect to w1 and w2 respectively.                 # 损失值对于w1 和 w2的梯度\n",
        "  loss.backward()\n",
        "\n",
        "  # Update weights using gradient descent. For this step we just want to mutate    # 使用梯度下降更新梯度, 对于这个步骤我们只想就地转变w1和w2的值\n",
        "  # the values of w1 and w2 in-place; we don't want to build up a computational   # 我们不想为更新步骤构建计算图, 所以我们使用上下文管理器torch.no_grad()\n",
        "  # graph for the update steps, so we use the torch.no_grad() context manager     ## 来阻止Pytorch 为了更新计算图而构建计算图\n",
        "  # to prevent PyTorch from building a computational graph for the updates       ##\n",
        "  with torch.no_grad():\n",
        "    w1 -= learning_rate * w1.grad\n",
        "    w2 -= learning_rate * w2.grad\n",
        "\n",
        "    # Manually zero the gradients after running the backward pass    # 运行后就要将梯度置为0\n",
        "    w1.grad.zero_()\n",
        "    w2.grad.zero_()"
      ],
      "metadata": {
        "id": "sccqmVyht4TJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PyTorch: Defining new autograd functions"
      ],
      "metadata": {
        "id": "IFo3i9qA-e6P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Code in file autograd/two_layer_net_custom_function.py\n",
        "import torch\n",
        "\n",
        "class MyReLU(torch.autograd.Function):\n",
        "  \"\"\"\n",
        "  We can implement our own custom autograd Functions by subclassing            ## 我们可以通过子类化 torch.autograd函数 来实现我们自己的自定义\n",
        "  torch.autograd.Function and implementing the forward and backward passes          # autograd函数 和实现前向传播和反向传播操作张量。 \n",
        "  which operate on Tensors.\n",
        "  \"\"\"\n",
        "  @staticmethod\n",
        "  def forward(ctx, x):  # ctx 就是context 翻译成 \"上下文或者环境\" 常用于静态环境\n",
        "    \"\"\"\n",
        "    In the forward pass we receive a context object and a Tensor containing the      #  在前向传播中我们得到了一个上下文对象和一个包含输入的张量\n",
        "    input; we must return a Tensor containing the output, and we can use the       ##  我们必须返回一个包含输出的张量的输出， 并且我们能够使用这个\n",
        "    context object to cache objects for use in the backward pass.              ##  上下文对象去缓存用于反向传播的对象\n",
        "    \"\"\"\n",
        "    ctx.save_for_backward(x)   #保存给定张量以便用于调用backward()\n",
        "    return x.clamp(min=0)\n",
        "\n",
        "  @staticmethod\n",
        "  def backward(ctx, grad_output):\n",
        "    \"\"\"\n",
        "    In the backward pass we receive the context object and a Tensor containing      # 在反向传播过程中，我们得到一个上下文对象和一个包含损失对于正向传播输出的梯度张量\n",
        "    the gradient of the loss with respect to the output produced during the       ## 我们可以从上下文对象中检索对象， 并且必须计算和返回损失梯度\n",
        "    forward pass. We can retrieve cached data from the context object, and must\n",
        "    compute and return the gradient of the loss with respect to the input to the\n",
        "    forward function.\n",
        "    \"\"\"\n",
        "    x, = ctx.saved_tensors     # 访问在forward期间保存在save_for_backward的值\n",
        "    grad_x = grad_output.clone()  # 深拷贝\n",
        "    grad_x[x < 0] = 0      \n",
        "    return grad_x\n",
        "\n",
        "#  上面的 forward 和 backward 有点感觉像是第一轮的东西\n",
        "\n",
        "\n",
        "device = torch.device('cpu')\n",
        "# device = torch.device('cuda') # Uncomment this to run on GPU\n",
        "\n",
        "# N is batch size; D_in is input dimension;\n",
        "# H is hidden dimension; D_out is output dimension.\n",
        "N, D_in, H, D_out = 64, 1000, 100, 10\n",
        "\n",
        "# Create random Tensors to hold input and output\n",
        "x = torch.randn(N, D_in, device=device)\n",
        "y = torch.randn(N, D_out, device=device)\n",
        "\n",
        "# Create random Tensors for weights.\n",
        "w1 = torch.randn(D_in, H, device=device, requires_grad=True)\n",
        "w2 = torch.randn(H, D_out, device=device, requires_grad=True)\n",
        "\n",
        "learning_rate = 1e-6\n",
        "for t in range(500):\n",
        "  # Forward pass: compute predicted y using operations on Tensors; we call our\n",
        "  # custom ReLU implementation using the MyReLU.apply function\n",
        "  y_pred = MyReLU.apply(x.mm(w1)).mm(w2)   # 这个apply 姑且就当做事传参用得， 网上讲的不是很清晰\n",
        " \n",
        "  # Compute and print loss\n",
        "  loss = (y_pred - y).pow(2).sum()\n",
        "  print(t, loss.item())\n",
        "\n",
        "  # Use autograd to compute the backward pass.\n",
        "  loss.backward()\n",
        "\n",
        "  with torch.no_grad():\n",
        "    # Update weights using gradient descent\n",
        "    w1 -= learning_rate * w1.grad\n",
        "    w2 -= learning_rate * w2.grad\n",
        "\n",
        "    # Manually zero the gradients after running the backward pass\n",
        "    w1.grad.zero_()\n",
        "    w2.grad.zero_()\n"
      ],
      "metadata": {
        "id": "nF376niC9bh5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color=\"#056DE8\" size=5>\n",
        "有个疑惑，就是在循环次数增大就有可能出现loss 提高又降低的情况\n",
        "</font>\n"
      ],
      "metadata": {
        "id": "8Pv1q7Tz0V8j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TensorFlow: Static Graphs"
      ],
      "metadata": {
        "id": "CLRFxHVG-oEa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Code in file autograd/tf_two_layer_net.py\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "# First we set up the computational graph:\n",
        "\n",
        "# N is batch size; D_in is input dimension;\n",
        "# H is hidden dimension; D_out is output dimension.\n",
        "N, D_in, H, D_out = 64, 1000, 100, 10\n",
        "\n",
        "# Create placeholders for the input and target data; these will be filled\n",
        "# with real data when we execute the graph.\n",
        "x = tf.placeholder(tf.float32, shape=(None, D_in))\n",
        "y = tf.placeholder(tf.float32, shape=(None, D_out))\n",
        "\n",
        "# Create Variables for the weights and initialize them with random data.\n",
        "# A TensorFlow Variable persists its value across executions of the graph.\n",
        "w1 = tf.Variable(tf.random_normal((D_in, H)))\n",
        "w2 = tf.Variable(tf.random_normal((H, D_out)))\n",
        "\n",
        "# Forward pass: Compute the predicted y using operations on TensorFlow Tensors.\n",
        "# Note that this code does not actually perform any numeric operations; it\n",
        "# merely sets up the computational graph that we will later execute.\n",
        "h = tf.matmul(x, w1)\n",
        "h_relu = tf.maximum(h, tf.zeros(1))\n",
        "y_pred = tf.matmul(h_relu, w2)\n",
        "\n",
        "# Compute loss using operations on TensorFlow Tensors\n",
        "loss = tf.reduce_sum((y - y_pred) ** 2.0)\n",
        "\n",
        "# Compute gradient of the loss with respect to w1 and w2.\n",
        "grad_w1, grad_w2 = tf.gradients(loss, [w1, w2])\n",
        "\n",
        "# Update the weights using gradient descent. To actually update the weights\n",
        "# we need to evaluate new_w1 and new_w2 when executing the graph. Note that\n",
        "# in TensorFlow the the act of updating the value of the weights is part of\n",
        "# the computational graph; in PyTorch this happens outside the computational\n",
        "# graph.\n",
        "learning_rate = 1e-6\n",
        "new_w1 = w1.assign(w1 - learning_rate * grad_w1)\n",
        "new_w2 = w2.assign(w2 - learning_rate * grad_w2)\n",
        "\n",
        "# Now we have built our computational graph, so we enter a TensorFlow session to\n",
        "# actually execute the graph.\n",
        "with tf.Session() as sess:\n",
        "  # Run the graph once to initialize the Variables w1 and w2.\n",
        "  sess.run(tf.global_variables_initializer())\n",
        "\n",
        "  # Create numpy arrays holding the actual data for the inputs x and targets y\n",
        "  x_value = np.random.randn(N, D_in)\n",
        "  y_value = np.random.randn(N, D_out)\n",
        "  for _ in range(500):\n",
        "    # Execute the graph many times. Each time it executes we want to bind\n",
        "    # x_value to x and y_value to y, specified with the feed_dict argument.\n",
        "    # Each time we execute the graph we want to compute the values for loss,\n",
        "    # new_w1, and new_w2; the values of these Tensors are returned as numpy\n",
        "    # arrays.\n",
        "    loss_value, _, _ = sess.run([loss, new_w1, new_w2],\n",
        "                                feed_dict={x: x_value, y: y_value})\n",
        "    print(loss_value)"
      ],
      "metadata": {
        "id": "ivlxJf7M1z5Z"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}